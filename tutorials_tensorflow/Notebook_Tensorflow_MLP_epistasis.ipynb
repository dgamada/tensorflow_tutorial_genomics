{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to the NN API\n",
    "\n",
    "Tensorflow has a nice api that includes many functions that we can help to seamlessly build and train neural networks models. End-to-end, the components we need to build are:\n",
    " * NN model --  takes inputs and makes a prediction\n",
    " * loss function -- takes NN model predictions and labels and calculates loss\n",
    " * optimizer -- calculates gradients of loss w.r.t. model parameters and sets update rules\n",
    " * training loop -- cycle through data with mini-batches and update parameters\n",
    " \n",
    "Let's first discuss the first part, building the NN model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to MLP\n",
    "\n",
    "The multi-layer perceptron model is the standard neural network that first comes to mind when mentioning artificial neural networks.  It is a feed-forward neural network that takes as input a vector of input features, the has one or more hidden layers, with the number of neurons set by your heart's desire, followed by an output layer to make a prediction. If we are doing a simple linear regression, we just need one output neuron.  If we are classifying 10 different objects for a computer vision task, then we need 10 output neurons.  \n",
    "\n",
    "Each hidden layer consist of a fully-connected layer, also known as a dense layer, that takes each of its inputs and calculates a weighted-sum to each hidden neuron.  Importantly, the weighted sum is then processed with a non-linear activation.  It turns out, theoretically, what this non-linear activation doesn't matter much. In practice, however, certain non-linear activations, like the rectified linear unit (ReLU), are easier to work with, making it easier to train deep neural networks. \n",
    "\n",
    "The MLP architecture has been well-characterized in terms of its expressive power and approximation capabilities.  Given enough hidden units, this kind of network can be considered a universal function approximator mapping the relationship between the input and the outputs.  \n",
    "\n",
    "Let's build some basic components of a MLP!\n",
    "\n",
    "Suppose we have 4 different mutations and we have some experimentally-determined fitness value. One thing we can do with thid data is to see if a MLP can learn the relationship between fitness and the combination of mutations. This data is taken directly from:\n",
    "\n",
    "Wu et. al. \"Adaptation in protein fitness landscapes is facilitated by indirect paths\" eLife, 2016.\n",
    "\n",
    "### load fitness data and training labels\n",
    "\n",
    "load the data from the supplemental file.  Convert the 4 mutations into a one-hot representation and normalize the fitness scores to a z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "filename = '../data/elife-16965-supp1-v4.xlsx'\n",
    "df = pd.read_excel(filename)\n",
    "\n",
    "# parse variants and fitness\n",
    "variants = df['Variants'].as_matrix()\n",
    "fitness = df['Fitness'].as_matrix()\n",
    "\n",
    "# filter data with a fitness of 0\n",
    "index = np.where(fitness!=0)[0]\n",
    "variants = variants[index]\n",
    "fitness = fitness[index]\n",
    "\n",
    "# create a one-hot vector for each variant\n",
    "alphabet = 'ACEDGFIHKMLNQPSRTWVY'\n",
    "num_alphabet = len(alphabet)\n",
    "num_variants = len(variants)\n",
    "one_hot = np.zeros((num_variants, num_alphabet, 4))\n",
    "for i in range(num_variants):\n",
    "    seq = str(variants[i])\n",
    "    for j in range(len(seq)):\n",
    "        index = alphabet.index(seq[j])\n",
    "        one_hot[i,index,j] = 1\n",
    "        \n",
    "# normalize fitness\n",
    "mu = np.mean(fitness)\n",
    "sigma = np.std(fitness)\n",
    "fitness = (fitness-mu)/sigma\n",
    "\n",
    "# convert to float 32 for GPU analysis\n",
    "one_hot = one_hot.astype(np.float32)\n",
    "fitness = fitness.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset into a training set, validation set, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "# split dataset into training cross validation and testing\n",
    "train_size = 0.65\n",
    "valid_size = 0.1\n",
    "test_size = 0.25\n",
    "split_size = [train_size, valid_size, test_size]\n",
    "\n",
    "# generate a shuffled subset of data for train, validation, and test\n",
    "num_seq = len(fitness)\n",
    "cum_index = np.cumsum(np.multiply([0, split_size[0], split_size[1], split_size[2]], num_seq)).astype(int) \n",
    "\n",
    "# randomly shuffle dataset\n",
    "shuffle = np.random.permutation(num_seq)\n",
    "\n",
    "# get shuffled indices for each data set\n",
    "train_index = shuffle[range(cum_index[0], cum_index[1])]\n",
    "valid_index = shuffle[range(cum_index[1], cum_index[2])]\n",
    "test_index = shuffle[range(cum_index[2], cum_index[3])]\n",
    "\n",
    "# create subsets of data based on indices \n",
    "X_train = one_hot[train_index]\n",
    "Y_train = np.expand_dims(fitness[train_index], axis=1)\n",
    "X_valid = one_hot[valid_index]\n",
    "Y_valid = np.expand_dims(fitness[valid_index], axis=1)\n",
    "X_test = one_hot[test_index]\n",
    "Y_test = np.expand_dims(fitness[test_index], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build MLP model\n",
    "\n",
    "### Input layer\n",
    "\n",
    "Next, we need to establish placeholders to be able to feed in input features into the MLP model. Since the data has an ndarray shape of 3, due to the one-hot representation, we need to flatten it such that it is a vector.  We can do this as a preprocessing step in numpy or we can include this reshape within the tensorflow graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholder\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, num_alphabet, 4])\n",
    "\n",
    "# reshape the inputs to be flat\n",
    "input_layer = tf.reshape(inputs, [-1, 4*num_alphabet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here x and y_ aren't specific values. Rather, they are each a placeholder -- a value that we'll input when we ask TensorFlow to run a computation. shape=None means that tensors of any shape will be accepted. Using shape=None is easy to construct graphs, but nightmarish for debugging. You should always define the shape of your placeholders as detailed as possible. \n",
    "\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "To create this model, we're going to need to create a lot of weights and biases. One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons\". Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us.\n",
    "\n",
    "In this most common case, the weights are initialized with the tf.truncated_normal and given their shape of a 2-D tensor with the first dim representing the number of units in the layer from which the weights connect and the second dim representing the number of units in the layer to which the weights connect. For the first layer, named hidden1, the dimensions are [IMAGE_PIXELS, hidden1_units] because the weights are connecting the image inputs to the hidden1 layer. The tf.truncated_normal initializer generates a random distribution with a given mean and standard deviation.\n",
    "\n",
    "Then the biases are initialized with tf.zeros to ensure they start with all zero values, and their shape is simply the number of units in the layer to which they connect.\n",
    "\n",
    "The graph's three primary ops -- two tf.nn.relu ops wrapping tf.matmul for the hidden layers and one extra tf.matmul for the logits -- are then created, each in turn, with separate tf.Variable instances connected to each of the input placeholders or the output tensors of the previous layer.\n",
    "\n",
    "\n",
    "  Now, we are going to be instantiating tensorflow variables.  To do so, we need to initialize them.  A standard initialization used for neural networks is He-inits and Glorot-inits.  Here, we will use he inits.  Let's define a function for He-inits for the parameters and constant-inits for the bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for initializations also known as inits\n",
    "def he_init(shape):\n",
    "    \"\"\"He inits are designed for neural networks that employ ReLU activations.\"\"\"\n",
    "    # fan_in is the number of neurons going into the layer\n",
    "    stddev = np.sqrt(2.6/shape[0])\n",
    "    return tf.truncated_normal(shape=shape, mean=0.0, stddev=stddev, dtype=tf.float32)\n",
    "\n",
    "def const_init(shape, value=0.0):\n",
    "    \"\"\"constant inits are useful for bias terms\"\"\"\n",
    "    return tf.constant(shape=shape, value=value, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layer\n",
    "\n",
    "For more details:\n",
    "https://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "\n",
    "To help improve the results of our model, we also apply dropout regularization to our dense layer, using the dropout method in layers:\n",
    "\n",
    "dropout = tf.layers.dropout(\n",
    "    inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "Again, inputs specifies the input tensor, which is the output tensor from our dense layer (dense).\n",
    "\n",
    "The rate argument specifies the dropout rate; here, we use 0.4, which means 40% of the elements will be randomly dropped out during training.\n",
    "\n",
    "The training argument takes a boolean specifying whether or not the model is currently being run in training mode; dropout will only be performed if training is True. Here, we check if the mode passed to our model function cnn_model_fn is TRAIN mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build a fully-connected layer (also known as a dense layer)\n",
    "num_units = 256\n",
    "num_incoming = 4*num_alphabet\n",
    "W1 = tf.Variable(he_init(shape=[num_incoming, num_units]), name='W1')\n",
    "b1 = tf.Variable(const_init(shape=[num_units], value=0.01), name='b1')\n",
    "dense_1 = tf.matmul(input_layer, W1)\n",
    "\n",
    "# add bias term\n",
    "dense_1_bias = tf.nn.bias_add(dense_1, b1)\n",
    "\n",
    "# perform activation\n",
    "dense_1_active = tf.nn.relu(dense_1_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing. TensorFlow's tf.nn.dropout op automatically handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder for dropout keep probability\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# apply dropout to dense 1 layer\n",
    "dense_1_dropout = tf.nn.dropout(dense_1_active, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fully-connected output layer\n",
    "\n",
    "The output layer is a linear regression model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable for weights and biases\n",
    "W2 = tf.Variable(he_init(shape=[num_units, 1]), name='W2')\n",
    "b2 = tf.Variable(const_init(shape=[1], value=0.01), name='b2')\n",
    "\n",
    "# build a fully-connected layer (also known as a dense layer)\n",
    "dense_2 = tf.matmul(dense_1_dropout, W2)\n",
    "\n",
    "# add bias term\n",
    "predictions = tf.nn.bias_add(dense_2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Loss\n",
    "\n",
    "For both training and evaluation, we need to define a loss function that measures how closely the model's predictions match the target classes. This is usually a log-likelihood function for supervised learning.  The log-likelihood for a linear regression is a mean-squared error, with some basic assumptions of course.  \n",
    "\n",
    "We also need to be able to calculate the gradients of the loss function with respect to the parameters of the model, which is efficiently accomplished with backpropagation.  Then, we can update the weights with a scaled gradients, which is knownas gradient descent learning. \n",
    "\n",
    "Fortunately, tensorflow provides a nice wrappers to perform all of this with the tf.train API.  Within tf.train are numerous types of gradient descent optimizers, including traditional stochastic gradient descent (SGD), SGD with momentum, ADAGRAD, RMSPROP, ADAM, plus more.  We will use ADAM in this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate placeholder for labels \n",
    "targets = tf.placeholder(tf.float32, shape=[None, 1], name='targets')\n",
    "\n",
    "# calculate binary cross-entropy loss function\n",
    "squared_error = tf.square(predictions - targets)\n",
    "loss_function = tf.reduce_mean(squared_error)\n",
    "    \n",
    "# get a list of all trainable variables\n",
    "trainable_vars = tf.trainable_variables()\n",
    "\n",
    "# flatten and concatenate all parameters for L2/L1 regularization\n",
    "all_params = []\n",
    "for param in trainable_vars:\n",
    "    all_params = tf.concat([all_params, tf.reshape(param, [-1,])], axis=0)\n",
    "    \n",
    "# calculate L2 regularization\n",
    "l2_strength = 1e-5\n",
    "loss_function += tf.reduce_sum(tf.square(all_params))*l2_strength\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0003, \n",
    "                                   beta1=0.9, \n",
    "                                   beta2=0.999)\n",
    "update_vars = optimizer.minimize(loss_function, var_list=trainable_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the MLP\n",
    "\n",
    "### Create a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create session\n",
    "sess = tf.Session()\n",
    "\n",
    "# initialize all variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### batch iterator\n",
    "\n",
    "We want to train our CNN model with mini-batch stochastic gradient descent.  So, we need a way to generate mini-batches of data. An effective strategy for training is to shuffle the dataset between each epoch. So, let's define a function to generate shuffled mini-batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=128, shuffle=True):\n",
    "    \"\"\"function to generate mini-batches of the data\"\"\"\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx+batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype(np.float32), y[excerpt].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory: ../results/epistasis\n",
      "Epoch 1 of 50\n",
      "  training loss:\t\t0.905065\n",
      "  validation loss:\t\t0.662489\n",
      "  validation Pearson r:\t\t0.509828\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 2 of 50\n",
      "  training loss:\t\t0.744407\n",
      "  validation loss:\t\t0.570326\n",
      "  validation Pearson r:\t\t0.606186\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 3 of 50\n",
      "  training loss:\t\t0.623410\n",
      "  validation loss:\t\t0.471984\n",
      "  validation Pearson r:\t\t0.692996\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 4 of 50\n",
      "  training loss:\t\t0.516946\n",
      "  validation loss:\t\t0.398697\n",
      "  validation Pearson r:\t\t0.747844\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 5 of 50\n",
      "  training loss:\t\t0.440017\n",
      "  validation loss:\t\t0.347614\n",
      "  validation Pearson r:\t\t0.785984\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 6 of 50\n",
      "  training loss:\t\t0.382777\n",
      "  validation loss:\t\t0.308489\n",
      "  validation Pearson r:\t\t0.811483\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 7 of 50\n",
      "  training loss:\t\t0.342357\n",
      "  validation loss:\t\t0.277116\n",
      "  validation Pearson r:\t\t0.833516\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 8 of 50\n",
      "  training loss:\t\t0.312448\n",
      "  validation loss:\t\t0.251913\n",
      "  validation Pearson r:\t\t0.850220\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 9 of 50\n",
      "  training loss:\t\t0.287775\n",
      "  validation loss:\t\t0.232264\n",
      "  validation Pearson r:\t\t0.863171\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 10 of 50\n",
      "  training loss:\t\t0.263022\n",
      "  validation loss:\t\t0.215997\n",
      "  validation Pearson r:\t\t0.873767\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 11 of 50\n",
      "  training loss:\t\t0.245558\n",
      "  validation loss:\t\t0.203003\n",
      "  validation Pearson r:\t\t0.880614\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 12 of 50\n",
      "  training loss:\t\t0.232364\n",
      "  validation loss:\t\t0.192186\n",
      "  validation Pearson r:\t\t0.888396\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 13 of 50\n",
      "  training loss:\t\t0.218299\n",
      "  validation loss:\t\t0.183682\n",
      "  validation Pearson r:\t\t0.892551\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 14 of 50\n",
      "  training loss:\t\t0.209850\n",
      "  validation loss:\t\t0.175416\n",
      "  validation Pearson r:\t\t0.897331\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 15 of 50\n",
      "  training loss:\t\t0.200826\n",
      "  validation loss:\t\t0.167073\n",
      "  validation Pearson r:\t\t0.902978\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 16 of 50\n",
      "  training loss:\t\t0.194355\n",
      "  validation loss:\t\t0.161016\n",
      "  validation Pearson r:\t\t0.906214\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 17 of 50\n",
      "  training loss:\t\t0.186119\n",
      "  validation loss:\t\t0.155051\n",
      "  validation Pearson r:\t\t0.909691\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 18 of 50\n",
      "  training loss:\t\t0.178024\n",
      "  validation loss:\t\t0.149570\n",
      "  validation Pearson r:\t\t0.912980\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 19 of 50\n",
      "  training loss:\t\t0.174553\n",
      "  validation loss:\t\t0.144429\n",
      "  validation Pearson r:\t\t0.916729\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 20 of 50\n",
      "  training loss:\t\t0.168951\n",
      "  validation loss:\t\t0.140441\n",
      "  validation Pearson r:\t\t0.919071\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 21 of 50\n",
      "  training loss:\t\t0.162937\n",
      "  validation loss:\t\t0.135546\n",
      "  validation Pearson r:\t\t0.921897\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 22 of 50\n",
      "  training loss:\t\t0.158225\n",
      "  validation loss:\t\t0.130766\n",
      "  validation Pearson r:\t\t0.925166\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 23 of 50\n",
      "  training loss:\t\t0.152023\n",
      "  validation loss:\t\t0.125932\n",
      "  validation Pearson r:\t\t0.927994\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 24 of 50\n",
      "  training loss:\t\t0.149317\n",
      "  validation loss:\t\t0.122918\n",
      "  validation Pearson r:\t\t0.929753\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 25 of 50\n",
      "  training loss:\t\t0.147019\n",
      "  validation loss:\t\t0.120697\n",
      "  validation Pearson r:\t\t0.930613\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 26 of 50\n",
      "  training loss:\t\t0.144588\n",
      "  validation loss:\t\t0.117099\n",
      "  validation Pearson r:\t\t0.933745\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 27 of 50\n",
      "  training loss:\t\t0.139846\n",
      "  validation loss:\t\t0.114363\n",
      "  validation Pearson r:\t\t0.934850\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 28 of 50\n",
      "  training loss:\t\t0.135381\n",
      "  validation loss:\t\t0.112329\n",
      "  validation Pearson r:\t\t0.935794\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 29 of 50\n",
      "  training loss:\t\t0.138768\n",
      "  validation loss:\t\t0.110786\n",
      "  validation Pearson r:\t\t0.937468\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 30 of 50\n",
      "  training loss:\t\t0.138193\n",
      "  validation loss:\t\t0.107019\n",
      "  validation Pearson r:\t\t0.939664\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 31 of 50\n",
      "  training loss:\t\t0.130116\n",
      "  validation loss:\t\t0.106182\n",
      "  validation Pearson r:\t\t0.941172\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 32 of 50\n",
      "  training loss:\t\t0.132257\n",
      "  validation loss:\t\t0.103308\n",
      "  validation Pearson r:\t\t0.941123\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 33 of 50\n",
      "  training loss:\t\t0.123925\n",
      "  validation loss:\t\t0.101749\n",
      "  validation Pearson r:\t\t0.942491\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 34 of 50\n",
      "  training loss:\t\t0.127516\n",
      "  validation loss:\t\t0.100057\n",
      "  validation Pearson r:\t\t0.944006\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 35 of 50\n",
      "  training loss:\t\t0.122334\n",
      "  validation loss:\t\t0.099081\n",
      "  validation Pearson r:\t\t0.943772\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 36 of 50\n",
      "  training loss:\t\t0.124646\n",
      "  validation loss:\t\t0.097166\n",
      "  validation Pearson r:\t\t0.944690\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 37 of 50\n",
      "  training loss:\t\t0.121868\n",
      "  validation loss:\t\t0.095972\n",
      "  validation Pearson r:\t\t0.945868\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 38 of 50\n",
      "  training loss:\t\t0.122326\n",
      "  validation loss:\t\t0.094437\n",
      "  validation Pearson r:\t\t0.947525\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 39 of 50\n",
      "  training loss:\t\t0.116700\n",
      "  validation loss:\t\t0.093570\n",
      "  validation Pearson r:\t\t0.947131\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 40 of 50\n",
      "  training loss:\t\t0.117850\n",
      "  validation loss:\t\t0.092019\n",
      "  validation Pearson r:\t\t0.948498\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 41 of 50\n",
      "  training loss:\t\t0.118004\n",
      "  validation loss:\t\t0.091078\n",
      "  validation Pearson r:\t\t0.949293\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 42 of 50\n",
      "  training loss:\t\t0.117744\n",
      "  validation loss:\t\t0.089577\n",
      "  validation Pearson r:\t\t0.949494\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 43 of 50\n",
      "  training loss:\t\t0.112072\n",
      "  validation loss:\t\t0.088381\n",
      "  validation Pearson r:\t\t0.950436\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 44 of 50\n",
      "  training loss:\t\t0.115524\n",
      "  validation loss:\t\t0.088680\n",
      "  validation Pearson r:\t\t0.950507\n",
      "Epoch 45 of 50\n",
      "  training loss:\t\t0.114259\n",
      "  validation loss:\t\t0.087675\n",
      "  validation Pearson r:\t\t0.950698\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 46 of 50\n",
      "  training loss:\t\t0.110329\n",
      "  validation loss:\t\t0.086596\n",
      "  validation Pearson r:\t\t0.951452\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 of 50\n",
      "  training loss:\t\t0.108724\n",
      "  validation loss:\t\t0.085820\n",
      "  validation Pearson r:\t\t0.952241\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 48 of 50\n",
      "  training loss:\t\t0.110836\n",
      "  validation loss:\t\t0.085244\n",
      "  validation Pearson r:\t\t0.951851\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 49 of 50\n",
      "  training loss:\t\t0.110364\n",
      "  validation loss:\t\t0.084088\n",
      "  validation Pearson r:\t\t0.952664\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n",
      "Epoch 50 of 50\n",
      "  training loss:\t\t0.110257\n",
      "  validation loss:\t\t0.083850\n",
      "  validation Pearson r:\t\t0.953591\n",
      "  Lower validation loss found. Saving parameters to: ../results/epistasis/mlp\n"
     ]
    }
   ],
   "source": [
    "# path to save model parameters\n",
    "save_path = '../results/epistasis'\n",
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    print(\"making directory: \" + save_path)\n",
    "params_path = os.path.join(save_path, 'mlp')\n",
    "    \n",
    "    \n",
    "num_epochs = 50        # maximum number of epochs\n",
    "batch_size = 100         # mini-batch size for a parameter update\n",
    "patience = 10         # number of epochs to wait for a lower validation loss to be found. \n",
    "num_train_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "# training loop\n",
    "wait = 0\n",
    "min_loss = 1e10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # make a batch ganerator for the training data\n",
    "    train_batches = batch_generator(X_train, Y_train, batch_size, shuffle=True)\n",
    "\n",
    "    # loop over training batches\n",
    "    train_loss = 0\n",
    "    for index in range(num_train_batches):\n",
    "        # get a mini-batch\n",
    "        x_batch, y_batch = next(train_batches)\n",
    "        \n",
    "        # update over a mini-batch\n",
    "        loss, _ = sess.run([loss_function, update_vars], \n",
    "                           feed_dict={inputs: x_batch, \n",
    "                                      targets: y_batch, \n",
    "                                      keep_prob: 0.5}\n",
    "                          )\n",
    "        train_loss += loss\n",
    "        \n",
    "    # check progress on validation set\n",
    "    valid_loss, valid_predict = sess.run([loss_function, predictions], \n",
    "                                         feed_dict={inputs: X_valid, \n",
    "                                                    targets: Y_valid, \n",
    "                                                    keep_prob: 1.0}\n",
    "                                        )\n",
    "\n",
    "    # calculate performance metrics\n",
    "    pearsonr = np.corrcoef(Y_valid[:,0], valid_predict[:,0])[0][1]\n",
    "\n",
    "    print(\"Epoch {} of {}\".format(epoch+1, num_epochs))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss/num_train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(valid_loss))\n",
    "    print(\"  validation Pearson r:\\t\\t{:.6f}\".format(pearsonr))\n",
    "    \n",
    "    \n",
    "    # check if current validation loss is lower, if so, save parameters, if not check patience\n",
    "    if valid_loss < min_loss:\n",
    "        print(\"  Lower validation loss found. Saving parameters to: \"+params_path)\n",
    "        \n",
    "        # save model parameters\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, save_path=params_path)\n",
    "        \n",
    "        # set minimum loss to the current validation loss\n",
    "        min_loss = valid_loss\n",
    "        \n",
    "        # reset wait time\n",
    "        wait = 0\n",
    "    else:\n",
    "        \n",
    "        # add to wait time\n",
    "        wait += 1\n",
    "        \n",
    "        # check to see if patience has run out\n",
    "        if wait == patience:\n",
    "            print(\"Patience ran out... early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  let's close the session, so we can walk through an example of how to load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close tensorflow session (Note, the graph is still open)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we can restore the parameters to our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../results/epistasis/mlp\n"
     ]
    }
   ],
   "source": [
    "# create a new session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# path to save results\n",
    "save_path = '../results/epistasis'\n",
    "params_path = os.path.join(save_path, 'mlp')\n",
    "\n",
    "# restore trained parameters\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, save_path=params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's test our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  validation loss:\t\t0.079236\n",
      "  validation Pearson r:\t\t0.963289\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "test_loss, test_predict = sess.run([loss_function, predictions], \n",
    "                                     feed_dict={inputs: X_test, \n",
    "                                                targets: Y_test, \n",
    "                                                keep_prob: 1.0})\n",
    "\n",
    "# calculate performance metrics\n",
    "pearsonr = np.corrcoef(Y_test[:,0], test_predict[:,0])[0][1]\n",
    "\n",
    "\n",
    "print(\"  validation loss:\\t\\t{:.6f}\".format(test_loss))\n",
    "print(\"  validation Pearson r:\\t\\t{:.6f}\".format(pearsonr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scatter plot of prediction and experimental values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7e3cb31850>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAETCAYAAADOPorfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e3f929290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(test_predict[:,0], Y_test[:,0], alpha=0.2)\n",
    "plt.xlabel('predictions', fontsize=14)\n",
    "plt.ylabel('experiment', fontsize=14)\n",
    "plt.plot([-5,20],[-5,20],'--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's generate a saliency map\n",
    "\n",
    "To calculate a saliency map, we need to calculate the derivative of the output neuron with respect to the inputs.  Luckily, tensorflow makes this easy because it calculates gradients using automatic differentiation.  We can calculate the gradient with the function tf.gradients.  This method constructs partial derivatives of ys w.r.t. x in xs. ys and xs are each a Tensor or a list of tensors. \n",
    "\n",
    "tf.gradients(\n",
    "    ys,\n",
    "    xs,\n",
    "    grad_ys=None,\n",
    "    name='gradients',\n",
    "    colocate_gradients_with_ops=False,\n",
    "    gate_gradients=False,\n",
    "    aggregation_method=None,\n",
    "    stop_gradients=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIcAAAD7CAYAAABAB4ewAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nODyxeHF4YvHi8MTixeGJxYvDE8v/BxW0Adq+xZuqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7dfc136f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's get the highest predicted sequence \n",
    "max_index = np.argsort(test_predict[:,0])[::-1]\n",
    "\n",
    "# get sequence\n",
    "index = 2\n",
    "X = np.expand_dims(X_test[max_index[index]], axis=0)\n",
    "\n",
    "# calculate the gradients to the inputs\n",
    "grad_to_inputs = tf.gradients(predictions, inputs)\n",
    "\n",
    "# run the session to calculate the saliency map for a given sequence X\n",
    "backprop_saliency = sess.run(grad_to_inputs, feed_dict={inputs: X, \n",
    "                                                        keep_prob: 1.0})\n",
    "\n",
    "\n",
    "# plot a heat map of the saliency values (each row is a different amino acid from the one-hot definition)\n",
    "plt.figure()\n",
    "plt.imshow(backprop_saliency[0][0], interpolation='None', cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.yticks(range(20), alphabet)\n",
    "plt.xticks(range(4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to go from here\n",
    "\n",
    "###  official tutorials\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "\n",
    "### lower-level APIs\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn\n",
    "\n",
    "\n",
    "### Layers \n",
    "    \n",
    "https://www.tensorflow.org/api_docs/python/tf/layers\n",
    "  \n",
    "* tf.nn.layers.conv2d()\n",
    "* tf.nn.layers.dense\n",
    "* tf.nn.layers.dropout\n",
    "* tf.nn.layers.max_pooling2d\n",
    "* tf.nn.layers.batch_normalization\n",
    "\n",
    "\n",
    "### alternative \n",
    "\n",
    "* tf.contrib.layers\n",
    "* tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
