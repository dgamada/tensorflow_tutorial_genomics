{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Convolutional Neural Networks\n",
    "\n",
    "Convolutional neural networks (CNNs) are the current state-of-the-art model architecture for image classification tasks. CNNs apply a series of filters to the raw pixel data of an image to extract and learn higher-level features, which the model can then use for classification. CNNs contains three components:\n",
    "\n",
    "* Convolutional layers, which apply a specified number of convolution filters to the image. For each subregion, the layer performs a set of mathematical operations to produce a single value in the output feature map. Convolutional layers then typically apply a ReLU activation function to the output to introduce nonlinearities into the model.\n",
    "\n",
    "* Pooling layers, which downsample the image data extracted by the convolutional layers to reduce the dimensionality of the feature map in order to decrease processing time. A commonly used pooling algorithm is max pooling, which extracts subregions of the feature map (e.g., 2x2-pixel tiles), keeps their maximum value, and discards all other values.\n",
    "\n",
    "* Dense (fully connected) layers, which perform classification on the features extracted by the convolutional layers and downsampled by the pooling layers. In a dense layer, every node in the layer is connected to every node in the preceding layer.\n",
    "\n",
    "Typically, a CNN is composed of a stack of convolutional modules that perform feature extraction. Each module consists of a convolutional layer followed by a pooling layer. The last convolutional module is followed by one or more dense layers that perform classification. The final dense layer in a CNN contains a single node for each target class in the model (all the possible classes the model may predict), with a softmax activation function to generate a value between 0–1 for each node (the sum of all these softmax values is equal to 1). We can interpret the softmax values for a given image as relative measurements of how likely it is that the image falls into each target class.\n",
    "\n",
    "https://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the CNN MNIST Classifier\n",
    "\n",
    "Let's build a model to classify the images in the MNIST dataset using the following CNN architecture:\n",
    "\n",
    "* Convolutional Layer #1: Applies 32 5x5 filters (extracting 5x5-pixel subregions), with ReLU activation function\n",
    "* Pooling Layer #1: Performs max pooling with a 2x2 filter and stride of 2 (which specifies that pooled regions do not overlap)\n",
    "* Convolutional Layer #2: Applies 64 5x5 filters, with ReLU activation function\n",
    "* Pooling Layer #2: Again, performs max pooling with a 2x2 filter and stride of 2\n",
    "* Dense Layer #1: 1,024 neurons, with dropout regularization rate of 0.4 (probability of 0.4 that any given element will be dropped during training)\n",
    "* Dense Layer #2 (Logits Layer): 10 neurons, one for each digit target class (0–9).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.layers module contains methods to create each of the three layer types above:\n",
    "\n",
    "* conv2d(). Constructs a two-dimensional convolutional layer. Takes number of filters, filter kernel size, padding, and activation function as arguments.\n",
    "* max_pooling2d(). Constructs a two-dimensional pooling layer using the max-pooling algorithm. Takes pooling filter size and stride as arguments.\n",
    "* dense(). Constructs a dense layer. Takes number of neurons and activation function as arguments.\n",
    "\n",
    "Each of these methods accepts a tensor as input and returns a transformed tensor as output. This makes it easy to connect one layer to another: just take the output from one layer-creation method and supply it as input to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders\n",
    "\n",
    "We start building the computation graph by creating nodes for the input images and target output classes.\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "Here x and y_ aren't specific values. Rather, they are each a placeholder -- a value that we'll input when we ask TensorFlow to run a computation.\n",
    "\n",
    "Dtype, shape, and name are self-explanatory. The only thing to note here is when you set the shape of the placeholder to None. shape=None means that tensors of any shape will be accepted. Using shape=None is easy to construct graphs, but nightmarish for debugging. You should always define the shape of your placeholders as detailed as possible. shape=None also breaks all following shape inference, which makes many ops not work because they expect certain rank. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The input images x will consist of a 2d tensor of floating point numbers. Here we assign it a shape of [None, 784], where 784 is the dimensionality of a single flattened 28 by 28 pixel MNIST image, and None indicates that the first dimension, corresponding to the batch size, can be of any size. The target output classes y_ will also consist of a 2d tensor, where each row is a one-hot 10-dimensional vector indicating which digit class (zero through nine) the corresponding MNIST image belongs to.\n",
    "\n",
    "The shape argument to placeholder is optional, but it allows TensorFlow to automatically catch bugs stemming from inconsistent tensor shapes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Layer\n",
    "\n",
    "The methods in the layers module for creating convolutional and pooling layers for two-dimensional image data expect input tensors to have a shape of [batch_size, image_width, image_height, channels], defined as follows:\n",
    "\n",
    "batch_size. Size of the subset of examples to use when performing gradient descent during training.\n",
    "image_width. Width of the example images.\n",
    "image_height. Height of the example images.\n",
    "channels. Number of color channels in the example images. For color images, the number of channels is 3 (red, green, blue). For monochrome images, there is just 1 channel (black).\n",
    "Here, our MNIST dataset is composed of monochrome 28x28 pixel images, so the desired shape for our input layer is [batch_size, 28, 28, 1].\n",
    "\n",
    "To convert our input feature map (features) to this shape, we can perform the following reshape operation:\n",
    "\n",
    "input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "Note that we've indicated -1 for batch size, which specifies that this dimension should be dynamically computed based on the number of input values in features[\"x\"], holding the size of all other dimensions constant. This allows us to treat batch_size as a hyperparameter that we can tune. For example, if we feed examples into our model in batches of 5, features[\"x\"] will contain 3,920 values (one value for each pixel in each image), and input_layer will have a shape of [5, 28, 28, 1]. Similarly, if we feed examples in batches of 100, features[\"x\"] will contain 78,400 values, and input_layer will have a shape of [100, 28, 28, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initialization\n",
    "\n",
    "To create this model, we're going to need to create a lot of weights and biases. One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons\". Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us.\n",
    "\n",
    "In this most common case, the weights are initialized with the tf.truncated_normal and given their shape of a 2-D tensor with the first dim representing the number of units in the layer from which the weights connect and the second dim representing the number of units in the layer to which the weights connect. For the first layer, named hidden1, the dimensions are [IMAGE_PIXELS, hidden1_units] because the weights are connecting the image inputs to the hidden1 layer. The tf.truncated_normal initializer generates a random distribution with a given mean and standard deviation.\n",
    "\n",
    "Then the biases are initialized with tf.zeros to ensure they start with all zero values, and their shape is simply the number of units in the layer to which they connect.\n",
    "\n",
    "The graph's three primary ops -- two tf.nn.relu ops wrapping tf.matmul for the hidden layers and one extra tf.matmul for the logits -- are then created, each in turn, with separate tf.Variable instances connected to each of the input placeholders or the output tensors of the previous layer.\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layer #1\n",
    "\n",
    "In our first convolutional layer, we want to apply 32 5x5 filters to the input layer, with a ReLU activation function. We can use the conv2d() method in the layers module to create this layer as follows:\n",
    "\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=32,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu)\n",
    "The inputs argument specifies our input tensor, which must have the shape [batch_size, image_width, image_height, channels]. Here, we're connecting our first convolutional layer to input_layer, which has the shape [batch_size, 28, 28, 1].\n",
    "\n",
    "The filters argument specifies the number of filters to apply (here, 32), and kernel_size specifies the dimensions of the filters as [width, height] (here, [5, 5]).\n",
    "\n",
    "TIP: If filter width and height have the same value, you can instead specify a single integer for kernel_size—e.g., kernel_size=5.\n",
    "\n",
    "The padding argument specifies one of two enumerated values (case-insensitive): valid (default value) or same. To specify that the output tensor should have the same width and height values as the input tensor, we set padding=same here, which instructs TensorFlow to add 0 values to the edges of the input tensor to preserve width and height of 28. (Without padding, a 5x5 convolution over a 28x28 tensor will produce a 24x24 tensor, as there are 24x24 locations to extract a 5x5 tile from a 28x28 grid.)\n",
    "\n",
    "The activation argument specifies the activation function to apply to the output of the convolution. Here, we specify ReLU activation with tf.nn.relu.\n",
    "\n",
    "Our output tensor produced by conv2d() has a shape of [batch_size, 28, 28, 32]: the same width and height dimensions as the input, but now with 32 channels holding the output from each of the filters.\n",
    "\n",
    "Convolution and Pooling\n",
    "\n",
    "TensorFlow also gives us a lot of flexibility in convolution and pooling operations. How do we handle the boundaries? What is our stride size? In this example, we're always going to choose the vanilla version. Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input. Our pooling is plain old max pooling over 2x2 blocks. To keep our code cleaner, let's also abstract those operations into functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling Layer #1\n",
    "\n",
    "Next, we connect our first pooling layer to the convolutional layer we just created. We can use the max_pooling2d() method in layers to construct a layer that performs max pooling with a 2x2 filter and stride of 2:\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "Again, inputs specifies the input tensor, with a shape of [batch_size, image_width, image_height, channels]. Here, our input tensor is conv1, the output from the first convolutional layer, which has a shape of [batch_size, 28, 28, 32].\n",
    "\n",
    "The pool_size argument specifies the size of the max pooling filter as [width, height] (here, [2, 2]). If both dimensions have the same value, you can instead specify a single integer (e.g., pool_size=2).\n",
    "\n",
    "The strides argument specifies the size of the stride. Here, we set a stride of 2, which indicates that the subregions extracted by the filter should be separated by 2 pixels in both the width and height dimensions (for a 2x2 filter, this means that none of the regions extracted will overlap). If you want to set different stride values for width and height, you can instead specify a tuple or list (e.g., stride=[3, 6]).\n",
    "\n",
    "Our output tensor produced by max_pooling2d() (pool1) has a shape of [batch_size, 14, 14, 32]: the 2x2 filter reduces width and height by 50% each.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help improve the results of our model, we also apply dropout regularization to our dense layer, using the dropout method in layers:\n",
    "\n",
    "dropout = tf.layers.dropout(\n",
    "    inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "Again, inputs specifies the input tensor, which is the output tensor from our dense layer (dense).\n",
    "\n",
    "The rate argument specifies the dropout rate; here, we use 0.4, which means 40% of the elements will be randomly dropped out during training.\n",
    "\n",
    "The training argument takes a boolean specifying whether or not the model is currently being run in training mode; dropout will only be performed if training is True. Here, we check if the mode passed to our model function cnn_model_fn is TRAIN mode.\n",
    "\n",
    "Dropout\n",
    "\n",
    "To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing. TensorFlow's tf.nn.dropout op automatically handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling.1\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits Layer\n",
    "\n",
    "The final layer in our neural network is the logits layer, which will return the raw values for our predictions. We create a dense layer with 10 neurons (one for each target class 0–9), with linear activation (the default):\n",
    "\n",
    "logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "Our final output tensor of the CNN, logits, has shape [batch_size, 10].\n",
    "\n",
    "Generate Predictions\n",
    "\n",
    "The logits layer of our model returns our predictions as raw values in a [batch_size, 10]-dimensional tensor. Let's convert these raw values into two different formats that our model function can return:\n",
    "\n",
    "The predicted class for each example: a digit from 0–9.\n",
    "The probabilities for each possible target class for each example: the probability that the example is a 0, is a 1, is a 2, etc.\n",
    "For a given example, our predicted class is the element in the corresponding row of the logits tensor with the highest raw value. We can find the index of this element using the tf.argmax function:\n",
    "\n",
    "tf.argmax(input=logits, axis=1)\n",
    "The input argument specifies the tensor from which to extract maximum values—here logits. The axis argument specifies the axis of the input tensor along which to find the greatest value. Here, we want to find the largest value along the dimension with index of 1, which corresponds to our predictions (recall that our logits tensor has shape [batch_size, 10]).\n",
    "\n",
    "We can derive probabilities from our logits layer by applying softmax activation using tf.nn.softmax:\n",
    "\n",
    "tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Loss\n",
    "\n",
    "For both training and evaluation, we need to define a loss function that measures how closely the model's predictions match the target classes. For multiclass classification problems like MNIST, cross entropy is typically used as the loss metric. The following code calculates cross entropy when the model runs in either TRAIN or EVAL mode:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "In order to train our model, we need to define what it means for the model to be good. Well, actually, in machine learning we typically define what it means for a model to be bad. We call this the cost, or the loss, and it represents how far off our model is from our desired outcome. We try to minimize that error, and the smaller the error margin, the better our model is.\n",
    "\n",
    "One very common, very nice function to determine the loss of a model is called \"cross-entropy.\" Cross-entropy arises from thinking about information compressing codes in information theory but it winds up being an important idea in lots of areas, from gambling to machine learning. It's defined as:\n",
    "\n",
    "Hy′(y)=−∑iyi′log⁡(yi)\n",
    "\n",
    "Where y is our predicted probability distribution, and y′ is the true distribution (the one-hot vector with the digit labels). In some rough sense, the cross-entropy is measuring how inefficient our predictions are for describing the truth. Going into more detail about cross-entropy is beyond the scope of this tutorial, but it's well worth understanding.\n",
    "\n",
    "To implement cross-entropy we need to first add a new placeholder to input the correct answers:\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "Then we can implement the cross-entropy function, −∑y′log⁡(y):\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "First, tf.log computes the logarithm of each element of y. Next, we multiply each element of y_ with the corresponding element of tf.log(y). Then tf.reduce_sum adds the elements in the second dimension of y, due to the reduction_indices=[1] parameter. Finally, tf.reduce_mean computes the mean over all the examples in the batch.\n",
    "\n",
    "Note that in the source code, we don't use this formulation, because it is numerically unstable. Instead, we apply tf.nn.softmax_cross_entropy_with_logits on the unnormalized logits (e.g., we call softmax_cross_entropy_with_logits on tf.matmul(x, W) + b), because this more numerically stable function internally computes the softmax activation. In your code, consider using tf.nn.softmax_cross_entropy_with_logits instead.\n",
    "Now that we have defined our model and training loss function, it is straightforward to train using TensorFlow. Because TensorFlow knows the entire computation graph, it can use automatic differentiation to find the gradients of the loss with respect to each of the variables. TensorFlow has a variety of built-in optimization algorithms. For this example, we will use steepest gradient descent, with a step length of 0.5, to descend the cross entropy.\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "What TensorFlow actually did in that single line was to add new operations to the computation graph. These operations included ones to compute gradients, compute parameter update steps, and apply update steps to the parameters.\n",
    "\n",
    "The returned operation train_step, when run, will apply the gradient descent updates to the parameters. Training the model can therefore be accomplished by repeatedly running train_step.\n",
    "\n",
    "for _ in range(1000):\n",
    "  batch = mnist.train.next_batch(100)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "We load 100 training examples in each training iteration. We then run the train_step operation, using feed_dict to replace the placeholder tensors x and y_ with the training examples. Note that you can replace any tensor in your computation graph using feed_dict -- it's not restricted to just placeholders.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Configure the Training Op\n",
    "\n",
    "In the previous section, we defined loss for our CNN as the softmax cross-entropy of the logits layer and our labels. Let's configure our model to optimize this loss value during training. We'll use a learning rate of 0.001 and stochastic gradient descent as the optimization algorithm:\n",
    "\n",
    "if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "  train_op = optimizer.minimize(\n",
    "      loss=loss,\n",
    "      global_step=tf.train.get_global_step())\n",
    "  return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Evaluate the Model\n",
    "\n",
    "How well does this model do? To train and evaluate it we will use code that is nearly identical to that for the simple one layer SoftMax network above.\n",
    "\n",
    "The differences are that:\n",
    "\n",
    "We will replace the steepest gradient descent optimizer with the more sophisticated ADAM optimizer.\n",
    "We will include the additional parameter keep_prob in feed_dict to control the dropout rate.\n",
    "We will add logging to every 100th iteration in the training process.\n",
    "We will also use tf.Session rather than tf.InteractiveSession. This better separates the process of creating the graph (model specification) and the process of evaluating the graph (model fitting). It generally makes for cleaner code. The tf.Session is created within a with block so that it is automatically destroyed once the block is exited.\n",
    "\n",
    "Feel free to run this code. Be aware that it does 20,000 training iterations and may take a while (possibly up to half an hour), depending on your processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add evaluation metrics\n",
    "\n",
    "To add accuracy metric in our model, we define eval_metric_ops dict in EVAL mode as follows:\n",
    "\n",
    "eval_metric_ops = {\n",
    "    \"accuracy\": tf.metrics.accuracy(\n",
    "        labels=labels, predictions=predictions[\"classes\"])}\n",
    "return tf.estimator.EstimatorSpec(\n",
    "    mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "Evaluating Our Model\n",
    "\n",
    "How well does our model do?\n",
    "\n",
    "Well, first let's figure out where we predicted the correct label. tf.argmax is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. For example, tf.argmax(y,1) is the label our model thinks is most likely for each input, while tf.argmax(y_,1) is the correct label. We can use tf.equal to check if our prediction matches the truth.\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "That gives us a list of booleans. To determine what fraction are correct, we cast to floating point numbers and then take the mean. For example, [True, False, True, True] would become [1,0,1,1] which would become 0.75.\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "Finally, we ask for our accuracy on our test data.\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "This should be about 92%.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training and Test Data\n",
    "\n",
    "First, let's load our training and test data. Add a main() function to cnn_mnist.py with the following code:\n",
    "\n",
    "We store the training feature data (the raw pixel values for 55,000 images of hand-drawn digits) and training labels (the corresponding value from 0–9 for each image) as numpy arrays in train_data and train_labels, respectively. Similarly, we store the evaluation feature data (10,000 images) and evaluation labels in eval_data and eval_labels, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a Checkpoint\n",
    "\n",
    "In order to emit a checkpoint file that may be used to later restore a model for further training or evaluation, we instantiate a tf.train.Saver.\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "In the training loop, the tf.train.Saver.save method will periodically be called to write a checkpoint file to the training directory with the current values of all the trainable variables.\n",
    "\n",
    "saver.save(sess, FLAGS.train_dir, global_step=step)\n",
    "At some later point in the future, training might be resumed by using the tf.train.Saver.restore method to reload the model parameters.\n",
    "\n",
    "saver.restore(sess, FLAGS.train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up a Logging Hook\n",
    "\n",
    "Since CNNs can take a while to train, let's set up some logging so we can track progress during training. We can use TensorFlow's tf.train.SessionRunHook to create a tf.train.LoggingTensorHook that will log the probability values from the softmax layer of our CNN. Add the following to main():\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Mechanics 101\n",
    "\n",
    "Contents\n",
    "Tutorial Files\n",
    "Prepare the Data\n",
    "Download\n",
    "Inputs and Placeholders\n",
    "\n",
    "Code: tensorflow/examples/tutorials/mnist/\n",
    "\n",
    "The goal of this tutorial is to show how to use TensorFlow to train and evaluate a simple feed-forward neural network for handwritten digit classification using the (classic) MNIST data set. The intended audience for this tutorial is experienced machine learning users interested in using TensorFlow.\n",
    "\n",
    "These tutorials are not intended for teaching Machine Learning in general.\n",
    "\n",
    "Please ensure you have followed the instructions to install TensorFlow.\n",
    "\n",
    "Tutorial Files\n",
    "\n",
    "This tutorial references the following files:\n",
    "\n",
    "File\tPurpose\n",
    "mnist.py\tThe code to build a fully-connected MNIST model.\n",
    "fully_connected_feed.py\tThe main code to train the built MNIST model against the downloaded dataset using a feed dictionary.\n",
    "Simply run the fully_connected_feed.py file directly to start training:\n",
    "\n",
    "python fully_connected_feed.py\n",
    "Prepare the Data\n",
    "\n",
    "MNIST is a classic problem in machine learning. The problem is to look at greyscale 28x28 pixel images of handwritten digits and determine which digit the image represents, for all the digits from zero to nine.\n",
    "\n",
    "MNIST Digits\n",
    "\n",
    "For more information, refer to Yann LeCun's MNIST page or Chris Olah's visualizations of MNIST.\n",
    "\n",
    "Download\n",
    "\n",
    "At the top of the run_training() method, the input_data.read_data_sets() function will ensure that the correct data has been downloaded to your local training folder and then unpack that data to return a dictionary of DataSet instances.\n",
    "\n",
    "data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)\n",
    "NOTE: The fake_data flag is used for unit-testing purposes and may be safely ignored by the reader.\n",
    "\n",
    "Dataset\tPurpose\n",
    "data_sets.train\t55000 images and labels, for primary training.\n",
    "data_sets.validation\t5000 images and labels, for iterative validation of training accuracy.\n",
    "data_sets.test\t10000 images and labels, for final testing of trained accuracy.\n",
    "Inputs and Placeholders\n",
    "\n",
    "The placeholder_inputs() function creates two tf.placeholder ops that define the shape of the inputs, including the batch_size, to the rest of the graph and into which the actual training examples will be fed.\n",
    "\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                       mnist.IMAGE_PIXELS))\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "Further down, in the training loop, the full image and label datasets are sliced to fit the batch_size for each step, matched with these placeholder ops, and then passed into the sess.run() function using the feed_dict parameter.\n",
    "\n",
    "Build the Graph\n",
    "\n",
    "After creating placeholders for the data, the graph is built from the mnist.py file according to a 3-stage pattern: inference(), loss(), and training().\n",
    "\n",
    "inference() - Builds the graph as far as required for running the network forward to make predictions.\n",
    "loss() - Adds to the inference graph the ops required to generate loss.\n",
    "training() - Adds to the loss graph the ops required to compute and apply gradients.\n",
    "\n",
    "Inference\n",
    "\n",
    "The inference() function builds the graph as far as needed to return the tensor that would contain the output predictions.\n",
    "\n",
    "It takes the images placeholder as input and builds on top of it a pair of fully connected layers with ReLU activation followed by a ten node linear layer specifying the output logits.\n",
    "\n",
    "Each layer is created beneath a unique tf.name_scope that acts as a prefix to the items created within that scope.\n",
    "\n",
    "with tf.name_scope('hidden1'):\n",
    "Within the defined scope, the weights and biases to be used by each of these layers are generated into tf.Variable instances, with their desired shapes:\n",
    "\n",
    "weights = tf.Variable(\n",
    "    tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                        stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "    name='weights')\n",
    "biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                     name='biases')\n",
    "When, for instance, these are created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "\n",
    "Each variable is given initializer ops as part of their construction.\n",
    "\n",
    "In this most common case, the weights are initialized with the tf.truncated_normal and given their shape of a 2-D tensor with the first dim representing the number of units in the layer from which the weights connect and the second dim representing the number of units in the layer to which the weights connect. For the first layer, named hidden1, the dimensions are [IMAGE_PIXELS, hidden1_units] because the weights are connecting the image inputs to the hidden1 layer. The tf.truncated_normal initializer generates a random distribution with a given mean and standard deviation.\n",
    "\n",
    "Then the biases are initialized with tf.zeros to ensure they start with all zero values, and their shape is simply the number of units in the layer to which they connect.\n",
    "\n",
    "The graph's three primary ops -- two tf.nn.relu ops wrapping tf.matmul for the hidden layers and one extra tf.matmul for the logits -- are then created, each in turn, with separate tf.Variable instances connected to each of the input placeholders or the output tensors of the previous layer.\n",
    "\n",
    "hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "logits = tf.matmul(hidden2, weights) + biases\n",
    "Finally, the logits tensor that will contain the output is returned.\n",
    "\n",
    "Loss\n",
    "\n",
    "The loss() function further builds the graph by adding the required loss ops.\n",
    "\n",
    "First, the values from the labels_placeholder are converted to 64-bit integers. Then, a tf.nn.sparse_softmax_cross_entropy_with_logits op is added to automatically produce 1-hot labels from the labels_placeholder and compare the output logits from the inference() function with those 1-hot labels.\n",
    "\n",
    "labels = tf.to_int64(labels)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=labels, logits=logits, name='xentropy')\n",
    "It then uses tf.reduce_mean to average the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "\n",
    "loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "And the tensor that will then contain the loss value is returned.\n",
    "\n",
    "Note: Cross-entropy is an idea from information theory that allows us to describe how bad it is to believe the predictions of the neural network, given what is actually true. For more information, read the blog post Visual Information Theory (http://colah.github.io/posts/2015-09-Visual-Information/)\n",
    "Training\n",
    "\n",
    "The training() function adds the operations needed to minimize the loss via Gradient Descent.\n",
    "\n",
    "Firstly, it takes the loss tensor from the loss() function and hands it to a tf.summary.scalar, an op for generating summary values into the events file when used with a tf.summary.FileWriter (see below). In this case, it will emit the snapshot value of the loss every time the summaries are written out.\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "Next, we instantiate a tf.train.GradientDescentOptimizer responsible for applying gradients with the requested learning rate.\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "We then generate a single variable to contain a counter for the global training step and the tf.train.Optimizer.minimize op is used to both update the trainable weights in the system and increment the global step. This op is, by convention, known as the train_op and is what must be run by a TensorFlow session in order to induce one full step of training (see below).\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "Train the Model\n",
    "\n",
    "Once the graph is built, it can be iteratively trained and evaluated in a loop controlled by the user code in fully_connected_feed.py.\n",
    "\n",
    "The Graph\n",
    "\n",
    "At the top of the run_training() function is a python with command that indicates all of the built ops are to be associated with the default global tf.Graph instance.\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "A tf.Graph is a collection of ops that may be executed together as a group. Most TensorFlow uses will only need to rely on the single default graph.\n",
    "\n",
    "More complicated uses with multiple graphs are possible, but beyond the scope of this simple tutorial.\n",
    "\n",
    "The Session\n",
    "\n",
    "Once all of the build preparation has been completed and all of the necessary ops generated, a tf.Session is created for running the graph.\n",
    "\n",
    "sess = tf.Session()\n",
    "Alternately, a Session may be generated into a with block for scoping:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "The empty parameter to session indicates that this code will attach to (or create if not yet created) the default local session.\n",
    "\n",
    "Immediately after creating the session, all of the tf.Variable instances are initialized by calling tf.Session.run on their initialization op.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "The tf.Session.run method will run the complete subset of the graph that corresponds to the op(s) passed as parameters. In this first call, the init op is a tf.group that contains only the initializers for the variables. None of the rest of the graph is run here; that happens in the training loop below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model\n",
    "\n",
    "Now we're ready to train our model, which we can do by creating train_input_fn ans calling train() on mnist_classifier. Add the following to main():\n",
    "\n",
    "\n",
    "Train Loop\n",
    "\n",
    "After initializing the variables with the session, training may begin.\n",
    "\n",
    "The user code controls the training per step, and the simplest loop that can do useful training is:\n",
    "\n",
    "for step in xrange(FLAGS.max_steps):\n",
    "    sess.run(train_op)\n",
    "However, this tutorial is slightly more complicated in that it must also slice up the input data for each step to match the previously generated placeholders.\n",
    "\n",
    "Feed the Graph\n",
    "\n",
    "For each step, the code will generate a feed dictionary that will contain the set of examples on which to train for the step, keyed by the placeholder ops they represent.\n",
    "\n",
    "In the fill_feed_dict() function, the given DataSet is queried for its next batch_size set of images and labels, and tensors matching the placeholders are filled containing the next images and labels.\n",
    "\n",
    "images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,\n",
    "                                               FLAGS.fake_data)\n",
    "A python dictionary object is then generated with the placeholders as keys and the representative feed tensors as values.\n",
    "\n",
    "feed_dict = {\n",
    "    images_placeholder: images_feed,\n",
    "    labels_placeholder: labels_feed,\n",
    "}\n",
    "This is passed into the sess.run() function's feed_dict parameter to provide the input examples for this step of training.\n",
    "\n",
    "Check the Status\n",
    "\n",
    "The code specifies two values to fetch in its run call: [train_op, loss].\n",
    "\n",
    "for step in xrange(FLAGS.max_steps):\n",
    "    feed_dict = fill_feed_dict(data_sets.train,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    _, loss_value = sess.run([train_op, loss],\n",
    "                             feed_dict=feed_dict)\n",
    "Because there are two values to fetch, sess.run() returns a tuple with two items. Each Tensor in the list of values to fetch corresponds to a numpy array in the returned tuple, filled with the value of that tensor during this step of training. Since train_op is an Operation with no output value, the corresponding element in the returned tuple is None and, thus, discarded. However, the value of the loss tensor may become NaN if the model diverges during training, so we capture this value for logging.\n",
    "\n",
    "Assuming that the training runs fine without NaNs, the training loop also prints a simple status text every 100 steps to let the user know the state of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low-level\n",
    "https://www.tensorflow.org/get_started/mnist/pros#build_a_multilayer_convolutional_network\n",
    "\n",
    "# high-level \n",
    "https://www.tensorflow.org/tutorials/layers\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/aymericdamien/TensorFlow-Examples\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Dense Layer\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits Layer\n",
    "  logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower-level APIs\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn\n",
    "    \n",
    "    \n",
    "tf.nn.conv2d\n",
    "tf.nn.dropout\n",
    "tf.nn.max_pool\n",
    "\n",
    "tf.nn.batch_normalization\n",
    "tf.nn.dilation2d\n",
    "tf.nn.embedding_lookup\n",
    "tf.nn.raw_nn\n",
    "tf.nn.dynamic_rnn\n",
    "\n",
    "\n",
    "tf.nn.sigmoid\n",
    "tf.nn.redlu\n",
    "tf.nn.tanh\n",
    "tf.nn.softmax\n",
    "tf.nn.softplus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/layers\n",
    "    \n",
    "# higher-level APIs\n",
    "tf.nn.layers.conv2d()\n",
    "tf.nn.layers.dense\n",
    "tf.nn.layers.dropout\n",
    "tf.nn.layers.max_pooling2d\n",
    "tf.nn.layers.batch_normalization\n",
    "\n",
    "# alternative \n",
    "tf.contrib.layers is same \n",
    "\n",
    "\n",
    "tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.tensorflow.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPUs\n",
    "\n",
    "On a typical system, there are multiple computing devices. In TensorFlow, the supported device types are CPU and GPU. They are represented as strings. For example:\n",
    "\n",
    "* \"/cpu:0\": The CPU of your machine.\n",
    "* \"/device:GPU:0\": The GPU of your machine, if you have one.\n",
    "* \"/device:GPU:1\": The second GPU of your machine, etc.\n",
    "\n",
    "If a TensorFlow operation has both CPU and GPU implementations, the GPU devices will be given priority when the operation is assigned to a device. For example, matmul has both CPU and GPU kernels. On a system with devices cpu:0 and gpu:0, gpu:0 will be selected to run matmul.\n",
    "\n",
    "To find out which devices your operations and tensors are assigned to, create the session with log_device_placement configuration option set to True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K40c, pci bus\n",
    "id: 0000:05:00.0\n",
    "b: /job:localhost/replica:0/task:0/device:GPU:0\n",
    "a: /job:localhost/replica:0/task:0/device:GPU:0\n",
    "MatMul: /job:localhost/replica:0/task:0/device:GPU:0\n",
    "[[ 22.  28.]\n",
    " [ 49.  64.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual device placement\n",
    "\n",
    "If you would like a particular operation to run on a device of your choice instead of what's automatically selected for you, you can use with tf.device to create a device context such that all the operations within that context will have the same device assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "with tf.device('/cpu:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that now a and b are assigned to cpu:0. Since a device was not explicitly specified for the MatMul operation, the TensorFlow runtime will choose one based on the operation and available devices (gpu:0 in this example) and automatically copy tensors between devices if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K40c, pci bus\n",
    "id: 0000:05:00.0\n",
    "b: /job:localhost/replica:0/task:0/cpu:0\n",
    "a: /job:localhost/replica:0/task:0/cpu:0\n",
    "MatMul: /job:localhost/replica:0/task:0/device:GPU:0\n",
    "[[ 22.  28.]\n",
    " [ 49.  64.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a single GPU on a multi-GPU system\n",
    "\n",
    "If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. If you would like to run on a different GPU, you will need to specify the preference explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:2'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
