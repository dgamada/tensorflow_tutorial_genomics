{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "\n",
    "Tensorboard is a graph visualization software included with any standard TensorFlow installation. The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, we've included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a user perform certain operations in a TensorFlow program, these operations are exported to an event log file. TensorBoard is able to convert these event files to visualizations that can give insight into a model's graph and its runtime behavior. \n",
    "\n",
    "To visualize the program with TensorBoard, we need to write log files of the program. To write event files, we first need to create a writer for those logs, using this code:\n",
    "\n",
    "writer = tf.summary.FileWriter([logdir], [graph])\n",
    "\n",
    "[graph] is the graph of the program you are working on. You can either call it using tf.get_default_graph(), which returns the default graph of the program, or through sess.graph, which returns the graph the session is handling. The latter requires you to already have created a session. Either way, make sure to create a writer only after youâ€™ve defined your graph, else the graph visualized on TensorBoard would be incomplete.\n",
    "\n",
    "[logdir] is the folder where you want to store those log files. You can choose [logdir] to be something meaningful such as './graphs'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "x = tf.add(a, b)\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "with tf.Session() as sess:\n",
    "    # writer = tf.summary.FileWriter('./graphs', sess.graph) # if you prefer creating your writer using session's graph\n",
    "    print(sess.run(x))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To launch tensorboard\n",
    "\n",
    "In a terminal, type:\n",
    "$ tensorboard --logdir=\"./graphs\" --port 6006  \n",
    "\n",
    "where logdir points to the directory where the FileWriter serialized its data. Once TensorBoard is running, navigate your web browser to:\n",
    "\n",
    "http://localhost:6006/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Naming elements of the graph\n",
    "You will see the TensorBoard page. Note that if you don't give variables/constants/ops a name, then tensorflow will automatically generate one for you.  So, we can go back and name the constants and see what the graph looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2, name=\"a\")\n",
    "b = tf.constant(2, name=\"b\")\n",
    "x = tf.add(a, b, name=\"add\")\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "with tf.Session() as sess:\n",
    "    # writer = tf.summary.FileWriter('./graphs', sess.graph) # if you prefer creating your writer using session's graph\n",
    "    print(sess.run(x))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning\n",
    "\n",
    "Notice that if you refresh tensorboard now, you will find two separate graphs.  You have to remember to reset the default graph if you want to maintain a single graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(2, name=\"a\")\n",
    "b = tf.constant(2, name=\"b\")\n",
    "x = tf.add(a, b, name=\"add\")\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "with tf.Session() as sess:\n",
    "    # writer = tf.summary.FileWriter('./graphs', sess.graph) # if you prefer creating your writer using session's graph\n",
    "    print(sess.run(x))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Name scoping and nodes\n",
    "\n",
    "Typical TensorFlow graphs can have many thousands of nodes--far too many to see easily all at once, or even to lay out using standard graph tools. To simplify, variable names can be scoped and the visualization uses this information to define a hierarchy on the nodes in the graph. By default, only the top of this hierarchy is shown. Here is an example that defines three operations under the hidden name scope using tf.name_scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "with tf.name_scope('hidden') as scope:\n",
    "    a = tf.constant(5, name='alpha')\n",
    "    W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0), name='weights')\n",
    "    b = tf.Variable(tf.zeros([1]), name='biases')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Grouping nodes by name scopes is critical to making a legible graph. If you're building a model, name scopes give you control over the resulting visualization. The better your name scopes, the better your visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Histogram Dashboard\n",
    "\n",
    "\n",
    "The TensorBoard Histogram Dashboard displays how the distribution of some Tensor in your TensorFlow graph has changed over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "k = tf.placeholder(tf.float32)\n",
    "\n",
    "# Make a normal distribution, with a shifting mean\n",
    "mean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)\n",
    "\n",
    "# Record that distribution into a histogram summary\n",
    "tf.summary.histogram(\"normal/moving_mean\", mean_moving_normal)\n",
    "\n",
    "# Setup a session and summary writer\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./histogram_example\")\n",
    "\n",
    "summaries = tf.summary.merge_all()\n",
    "\n",
    "# Setup a loop and write the summaries to disk\n",
    "N = 400\n",
    "for step in range(N):\n",
    "    k_val = step/float(N)\n",
    "    summ = sess.run(summaries, feed_dict={k: k_val})\n",
    "    writer.add_summary(summ, global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To launch tensorboard\n",
    "\n",
    "In a terminal, type:\n",
    "$ tensorboard --logdir=\"./histogram_example\" --port 6006  \n",
    "\n",
    "where logdir points to the directory where the FileWriter serialized its data. Once TensorBoard is running, navigate your web browser to:\n",
    "\n",
    "http://localhost:6006/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other notes\n",
    "\n",
    "#### dependencies\n",
    "\n",
    "TensorFlow graphs have two kinds of connections: data dependencies and control dependencies. Data dependencies show the flow of tensors between two ops and are shown as solid arrows, while control dependencies use dotted lines. \n",
    "\n",
    "#### chaning visuals\n",
    "\n",
    "TensorBoard provides several ways to change the visual layout of the graph. This doesn't change the graph's computational semantics, but it can bring some clarity to the network's structure. By right clicking on a node or pressing buttons on the bottom of that node's info card, you can make the following changes to its layout.\n",
    "\n",
    "### Runtime statistics\n",
    "\n",
    "Often it is useful to collect runtime metadata for a run, such as total memory usage, total compute time, and tensor shapes for nodes. \n",
    "\n",
    "The TensorBoard Histogram Dashboard displays how the distribution of some Tensor in your TensorFlow graph has changed over time. It does this by showing many histograms visualizations of your tensor at different points in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
